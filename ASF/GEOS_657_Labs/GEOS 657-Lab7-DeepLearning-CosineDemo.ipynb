{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.jpg\" width=\"100%\" />\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"7\"> <b> GEOS 657: Microwave Remote Sensing <b> </font>\n",
    "\n",
    "<font size=\"5\"> <b>Lab 7: Deep Learning in Earth Observation: Demo Exercise </b> </font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Lichao Mou, German Aerospace Center; Xiaoxiang Zhu, German Aerospace Center & Technical University Munich </b> <br>\n",
    "</font>\n",
    "\n",
    "<img src=\"NotebookAddons/dlr-logo-png-transparent.png\" width=\"170\" align=\"right\" border=\"2\"/> <font size=\"3\"> This Lab introduces you to the basic concepts of Deep Learning in Earth Observation. Specifically, it uses the simple example of learning the temporal pattern of a cosine curve to demonstrate the concepts of Recurrent Neural Networks (RNNs). The lab let's you experiment with several hyper-parameters needed for training Deep Learning Networks such as RNNs, CNNs, or similar.\n",
    "    \n",
    "We will again use a **Jupyter Notebook** framework implemented within the Amazon Web Services (AWS) cloud to work on this exercise. This Lab is part of the UAF course <a href=\"https://radar.community.uaf.edu/\" target=\"_blank\">GEOS 657: Microwave Remote Sensing</a>. It will introduce the following data analysis concepts:\n",
    "\n",
    "- How to set up a recurrent deep network within the Python-based <i>keras/tensorflow</i> environment\n",
    "- How to create an LSTM (long-term/short-term memory) recurrent network \n",
    "- How to optimize hyper-parameters when training a deep neural network\n",
    "</font>\n",
    "\n",
    "<font size=\"4\"> <font color='rgba(200,0,0,0.2)'> <b>There are no Homework assignments associated with this Notebook </b> </font>\n",
    "</font>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict a cosine wave using RNNs\n",
    "\n",
    "* A simple tutorial on LSTM and GRU to perdict a trigonometric wave.\n",
    "\n",
    "* Data noise can be added to test the robustness of the model.\n",
    "\n",
    "* Hyperparamters of the RNNs can be tweaked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, GRU, TimeDistributed\n",
    "from keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cosine data\n",
    "\n",
    "Data to train and evaluate the RNN:\n",
    "\n",
    "* Start, end and step define the range of the data series.\n",
    "* Sequence length defines the series to look back to train the model\n",
    "* Noisy data can be added to make the training data imperfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_data(start, end, step, sequence_length, noise_level=0):\n",
    "    '''\n",
    "    Define training data\n",
    "    :param start: Starting point\n",
    "    :param end: End point\n",
    "    :param step: Steps between points\n",
    "    :param sequence_length: Number of steps to backpropagate through time\n",
    "    :param noise_level: add noise to create imperfect data\n",
    "    :return: X,Y data\n",
    "    '''\n",
    "    t = np.arange(start, end, step)\n",
    "    cosine = np.cos(2 * np.pi * t) + noise_level*np.random.normal(0,1, np.shape(t))\n",
    "    cosine = cosine.reshape((cosine.shape[0], 1))\n",
    "    \n",
    "    dX, dY = [], []\n",
    "    for i in range(len(cosine) - 2*sequence_length):\n",
    "        dX.append(cosine[i:i + sequence_length])\n",
    "        dY.append(cosine[i + sequence_length:i + 2*sequence_length])\n",
    "    dataX = np.array(dX)\n",
    "    dataY = np.array(dY)\n",
    "    return dataX, dataY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a LSTM model\n",
    "\n",
    "* Linear activation\n",
    "* Loss in mean squared error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_(hidden_neurons, feature_count, learning_rate):\n",
    "    '''\n",
    "    Define a LSTM model\n",
    "    :param hidden_neurons: number of neurons to train a GRU network\n",
    "    :param feature_count: number of features to predict\n",
    "    :param learning_rate\n",
    "    :return: Return model for training\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(input_dim=feature_count, output_dim=hidden_neurons, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(feature_count)))\n",
    "    model.add(Activation('linear'))\n",
    "    optimizer = RMSprop(lr=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a GRU model\n",
    "\n",
    "* Linear activation\n",
    "* Get loss using a mean squared error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_(hidden_neurons, feature_count, learning_rate):\n",
    "    '''\n",
    "    Define a GRU model\n",
    "    :param hidden_neurons: number of neurons to train a GRU network\n",
    "    :param feature_count: number of features to predict\n",
    "    :param learning_rate \n",
    "    :return: Return model for training\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(GRU(input_dim=feature_count, output_dim=hidden_neurons, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(feature_count)))\n",
    "    model.add(Activation('linear'))\n",
    "    optimizer = RMSprop(lr=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train a RNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cosine(model, dataX, dataY, batch_size, epoch_count):\n",
    "    '''\n",
    "    :param model: load RNN model\n",
    "    :param dataX: X cosine train data\n",
    "    :param dataY: Y cosine train data\n",
    "    :param batch_size: Number of samples that going to be propagated through the network\n",
    "    :param epoch_count: Number of time dataset is processed\n",
    "    :return: training and validation loss\n",
    "    '''\n",
    "    history = model.fit(dataX, dataY, batch_size=batch_size, epochs=epoch_count, validation_split=0.05)\n",
    "    loss_history = history.history['loss']\n",
    "    loss_history = np.array(loss_history)\n",
    "    #np.savetxt(\"loss_history.txt\", numpy_loss_history, delimiter=\",\")\n",
    "    val_loss_history = history.history['val_loss']\n",
    "    val_loss_history = np.array(val_loss_history)\n",
    "    #np.savetxt(\"val_loss_history.txt\", numpy_loss_history, delimiter=\",\")\n",
    "    loss = history.history['loss']\n",
    "    loss_val = history.history['val_loss']\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    fig = plt.figure(figsize=(8,7))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    plt.plot(loss)\n",
    "    plt.plot(loss_val)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    return loss_history, val_loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run RNN code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cosine(EPOCHS, noise_level=0.3, sequence_length=100, learning_rate=1e-3, batch_size=16, nb_units=32, plot_results=False):\n",
    "    '''\n",
    "    Fuction to run a RNN at the specified parameters\n",
    "    :param EPOCHS: Number of Epochs\n",
    "    :param noise_level: Added noise level in training data\n",
    "    :param sequence_length: Sequence length available to train a RNN\n",
    "    :param learning_rate\n",
    "    :param batch_size\n",
    "    :param nb_units\n",
    "    :param plot_results: Boolean if results should be plotted\n",
    "    :return: Loss and Plot\n",
    "    '''\n",
    "    dataX, dataY = cosine_data(0.0, 10, 0.02, sequence_length, noise_level)  #4.0\n",
    "    # create and fit the LSTM network\n",
    "    print('creating model...')\n",
    "\n",
    "    # Choose RNN to train\n",
    "    model = LSTM_(nb_units, 1, learning_rate)\n",
    "    #model = GRU_(nb_units, 1, learning_rate)\n",
    "\n",
    "    # Train RNN model\n",
    "    tr_loss, val_loss = train_cosine(model, dataX, dataY, batch_size, EPOCHS)\n",
    "\n",
    "    # now test\n",
    "    dataX1, dataY1 = cosine_data(15.0, 21.0, 0.02, sequence_length)\n",
    "    predict = model.predict(dataX1)\n",
    "\n",
    "    # now plot\n",
    "    nan_array = np.empty((sequence_length - 1))\n",
    "    nan_array.fill(np.nan)\n",
    "    nan_array2 = np.empty(sequence_length)\n",
    "    nan_array2.fill(np.nan)\n",
    "    ind = np.arange(2*sequence_length)\n",
    "\n",
    "\n",
    "            \n",
    "    if plot_results == True:\n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "        fig = plt.figure(figsize=(8,7))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        #fig, ax = plt.subplots()\n",
    "        for i in range(0, sequence_length, sequence_length):\n",
    "            forecasts = np.concatenate((nan_array, dataX1[i, -1:, 0], predict[i, :, 0]))\n",
    "            ground_truth = np.concatenate((nan_array, dataX1[i, -1:, 0], dataY1[i, :, 0]))\n",
    "            network_input = np.concatenate((dataX[i, :, 0], nan_array2))\n",
    "\n",
    "            ax.plot(ind, network_input, 'b-x', label='Network input')\n",
    "            ax.plot(ind, forecasts, 'r-x', label='Many to many model forecast')\n",
    "            ax.plot(ind, ground_truth, 'g-x', label='Ground truth')\n",
    "\n",
    "            plt.xlabel('t')\n",
    "            plt.ylabel('cos(t)')\n",
    "            plt.title('Cosine Many to Many Forecast')\n",
    "            plt.legend(loc='best')\n",
    "            #plt.savefig(os.getcwd() + \"cosine_wave\" + str(i) + '.png')\n",
    "            plt.show()\n",
    "    return tr_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    nb_epochs = 10\n",
    "    noise_level = 0.0\n",
    "    sequence_length = 100\n",
    "    '''\n",
    "    learning_rate = 1e-3  # 1e-1, 1e-2, 1e-3, 1e-4, 1e-5\n",
    "    batch_size = 16  # 1, 2, 4, 8, 16\n",
    "    nb_units = 32  # 8, 16, 32, 64, 128\n",
    "\n",
    "    _, _ = test_cosine(EPOCHS = nb_epochs, noise_level =noise_level, \n",
    "                       plot_results=True)\n",
    "    '''\n",
    "    # try with different noise\n",
    "    '''\n",
    "    noise_level_range = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    for nl in noise_level_range:\n",
    "        tr_loss, val_loss = test_cosine(EPOCHS = nb_epochs, noise_level=nl)\n",
    "        print('tr_loss:', tr_loss)\n",
    "        print('val_loss:', val_loss)\n",
    "    '''\n",
    "    \n",
    "    # hyperparameter: 1) lr\n",
    "    learning_rate_range = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    for lr in learning_rate_range:\n",
    "        tr_loss, val_loss = test_cosine(EPOCHS = nb_epochs, learning_rate=lr, plot_results=True)\n",
    "        print('tr_loss:', tr_loss)\n",
    "        print('val_loss:', val_loss)\n",
    "    \n",
    "    '''\n",
    "    # hyperparameter: 2) batch size\n",
    "    batch_size_range = [2, 4, 8, 16, 32]\n",
    "    for bs in batch_size_range:\n",
    "        tr_loss, val_loss = test_cosine(EPOCHS = nb_epochs, batch_size=bs, plot_results=True)\n",
    "        print('tr_loss:', tr_loss)\n",
    "        print('val_loss:', val_loss)\n",
    "    '''\n",
    "    '''\n",
    "    # hyperparameter: 3) sequence length\n",
    "    sequence_length_range = [20, 50, 100, 200, 500]\n",
    "    for sl in sequence_length_range:\n",
    "        tr_loss, val_loss = test_cosine(EPOCHS = nb_epochs, sequence_length=sl, plot_results=True)\n",
    "        print('tr_loss:', tr_loss)\n",
    "        print('val_loss:', val_loss)\n",
    "    '''\n",
    "    '''\n",
    "    # hyperparameter: 4) nb_units\n",
    "    nb_units_range = [8, 16, 32, 64, 128]\n",
    "    for nu in nb_units_range:\n",
    "        tr_loss, val_loss = test_cosine(EPOCHS = nb_epochs, nb_units=nu, plot_results=True)\n",
    "        print('tr_loss:', tr_loss)\n",
    "        print('val_loss:', val_loss)\n",
    "    '''\n",
    "\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
